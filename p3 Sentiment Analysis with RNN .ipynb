{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxtfNhL4NiUV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fi1gDv7kNigz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoNkXKfebD50"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, Dense, Input, SimpleRNN, Dropout, LSTM, Bidirectional\n",
        "from tensorflow.keras import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uHg2-K3aM5Cn",
        "outputId": "3c7474b0-b730-49d2-b3df-0b86a7a5c422"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " \"he's\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " \"i've\",\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " \"we've\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from nltk import word_tokenize\n",
        "import pandas as pd\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqtS50x1u26-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-hiZy1sMz8W"
      },
      "outputs": [],
      "source": [
        "x = [\n",
        "    \"This movie was absolutely fantastic! The acting was superb and the plot kept me engaged throughout.\",\n",
        "    \"Terrible film. Poor acting and a boring storyline. Would not recommend to anyone.\",\n",
        "    \"Loved every minute of it! The cinematography was stunning and the characters were well-developed.\",\n",
        "    \"A complete waste of time. The plot made no sense and the dialogue was cringe-worthy.\",\n",
        "    \"One of the best movies I've seen this year. The emotional depth was incredible.\",\n",
        "    \"The acting was subpar and the story dragged on for too long. Very disappointing.\",\n",
        "    \"An amazing cinematic experience! The soundtrack perfectly complemented the visuals.\",\n",
        "    \"I couldn't even finish watching it. The characters were unlikeable and the pacing was awful.\",\n",
        "    \"A heartwarming story with brilliant performances. Definitely worth watching!\",\n",
        "    \"The worst movie I've seen in a long time. Poor direction and terrible special effects.\",\n",
        "    \"This film exceeded all my expectations. The plot twists were unexpected and exciting.\",\n",
        "    \"Boring from start to finish. I regret spending money on this movie.\",\n",
        "    \"The cast delivered outstanding performances. A must-see for all movie lovers!\",\n",
        "    \"The script was weak and the acting felt forced. Not enjoyable at all.\",\n",
        "    \"A masterpiece of modern cinema. The director's vision was executed perfectly.\",\n",
        "    \"I fell asleep halfway through. Utterly boring and predictable.\",\n",
        "    \"The visual effects were groundbreaking and the story was compelling.\",\n",
        "    \"Don't bother watching this. It's poorly written and badly acted.\",\n",
        "    \"An emotional rollercoaster that left me in tears. Beautiful film!\",\n",
        "    \"The movie was too long and lacked any real substance. Very overrated.\",\n",
        "\n",
        "]\n",
        "\n",
        "y = [\n",
        "    1,  # positive\n",
        "    0,  # negative\n",
        "    1,  # positive\n",
        "    0,  # negative\n",
        "    1,  # positive\n",
        "    0,  # negative\n",
        "    1,  # positive\n",
        "    0,  # negative\n",
        "    1,  # positive\n",
        "    0,  # negative\n",
        "    1,  # positive\n",
        "    0,  # negative\n",
        "    1,  # positive\n",
        "    0,  # negative\n",
        "    1,  # positive\n",
        "    0,  # negative\n",
        "    1,  # positive\n",
        "    0,  # negative\n",
        "    1,  # positive\n",
        "    0,\n",
        "       # negative\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSEyLnaFM_ln"
      },
      "outputs": [],
      "source": [
        "\n",
        "realtext = []\n",
        "\n",
        "for i in range(len(x)):\n",
        "    tokens = word_tokenize(x[i])\n",
        "    clean_tokens = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "    clean_tokens = [token for token in clean_tokens if token.lower() not in stopwords.words('english')]\n",
        "\n",
        "    clean_text = ' '.join(clean_tokens)\n",
        "\n",
        "    realtext.append(clean_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYAsJKeRq7es"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(realtext)\n",
        "\n",
        "sequence = tokenizer.texts_to_sequences(realtext)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OAakv6R-r_6r",
        "outputId": "6489caa9-f102-4ecd-ab66-478613d987cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'movie': 1,\n",
              " 'acting': 2,\n",
              " 'plot': 3,\n",
              " 'film': 4,\n",
              " 'boring': 5,\n",
              " 'story': 6,\n",
              " 'long': 7,\n",
              " 'watching': 8,\n",
              " 'terrible': 9,\n",
              " 'poor': 10,\n",
              " 'characters': 11,\n",
              " 'time': 12,\n",
              " \"'ve\": 13,\n",
              " 'seen': 14,\n",
              " 'emotional': 15,\n",
              " 'perfectly': 16,\n",
              " \"n't\": 17,\n",
              " 'finish': 18,\n",
              " 'performances': 19,\n",
              " 'effects': 20,\n",
              " \"'s\": 21,\n",
              " 'absolutely': 22,\n",
              " 'fantastic': 23,\n",
              " 'superb': 24,\n",
              " 'kept': 25,\n",
              " 'engaged': 26,\n",
              " 'throughout': 27,\n",
              " 'storyline': 28,\n",
              " 'would': 29,\n",
              " 'recommend': 30,\n",
              " 'anyone': 31,\n",
              " 'loved': 32,\n",
              " 'every': 33,\n",
              " 'minute': 34,\n",
              " 'cinematography': 35,\n",
              " 'stunning': 36,\n",
              " 'well': 37,\n",
              " 'developed': 38,\n",
              " 'complete': 39,\n",
              " 'waste': 40,\n",
              " 'made': 41,\n",
              " 'sense': 42,\n",
              " 'dialogue': 43,\n",
              " 'cringe': 44,\n",
              " 'worthy': 45,\n",
              " 'one': 46,\n",
              " 'best': 47,\n",
              " 'movies': 48,\n",
              " 'year': 49,\n",
              " 'depth': 50,\n",
              " 'incredible': 51,\n",
              " 'subpar': 52,\n",
              " 'dragged': 53,\n",
              " 'disappointing': 54,\n",
              " 'amazing': 55,\n",
              " 'cinematic': 56,\n",
              " 'experience': 57,\n",
              " 'soundtrack': 58,\n",
              " 'complemented': 59,\n",
              " 'visuals': 60,\n",
              " 'could': 61,\n",
              " 'even': 62,\n",
              " 'unlikeable': 63,\n",
              " 'pacing': 64,\n",
              " 'awful': 65,\n",
              " 'heartwarming': 66,\n",
              " 'brilliant': 67,\n",
              " 'definitely': 68,\n",
              " 'worth': 69,\n",
              " 'worst': 70,\n",
              " 'direction': 71,\n",
              " 'special': 72,\n",
              " 'exceeded': 73,\n",
              " 'expectations': 74,\n",
              " 'twists': 75,\n",
              " 'unexpected': 76,\n",
              " 'exciting': 77,\n",
              " 'start': 78,\n",
              " 'regret': 79,\n",
              " 'spending': 80,\n",
              " 'money': 81,\n",
              " 'cast': 82,\n",
              " 'delivered': 83,\n",
              " 'outstanding': 84,\n",
              " 'must': 85,\n",
              " 'see': 86,\n",
              " 'lovers': 87,\n",
              " 'script': 88,\n",
              " 'weak': 89,\n",
              " 'felt': 90,\n",
              " 'forced': 91,\n",
              " 'enjoyable': 92,\n",
              " 'masterpiece': 93,\n",
              " 'modern': 94,\n",
              " 'cinema': 95,\n",
              " 'director': 96,\n",
              " 'vision': 97,\n",
              " 'executed': 98,\n",
              " 'fell': 99,\n",
              " 'asleep': 100,\n",
              " 'halfway': 101,\n",
              " 'utterly': 102,\n",
              " 'predictable': 103,\n",
              " 'visual': 104,\n",
              " 'groundbreaking': 105,\n",
              " 'compelling': 106,\n",
              " 'bother': 107,\n",
              " 'poorly': 108,\n",
              " 'written': 109,\n",
              " 'badly': 110,\n",
              " 'acted': 111,\n",
              " 'rollercoaster': 112,\n",
              " 'left': 113,\n",
              " 'tears': 114,\n",
              " 'beautiful': 115,\n",
              " 'lacked': 116,\n",
              " 'real': 117,\n",
              " 'substance': 118,\n",
              " 'overrated': 119}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFTZUhA8tFCV",
        "outputId": "05e1d47f-6de2-4220-d655-6e65e33b14ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "max_len = max(len(i) for i in sequence)\n",
        "max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "g0xtWZNuuupa",
        "outputId": "507b17c0-f3b4-4174-eaf9-99713b30ebe1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  1,  22,  23,   2,  24,   3,  25,  26,  27,   0,   0],\n",
              "       [  9,   4,  10,   2,   5,  28,  29,  30,  31,   0,   0],\n",
              "       [ 32,  33,  34,  35,  36,  11,  37,  38,   0,   0,   0],\n",
              "       [ 39,  40,  12,   3,  41,  42,  43,  44,  45,   0,   0],\n",
              "       [ 46,  47,  48,  13,  14,  49,  15,  50,  51,   0,   0],\n",
              "       [  2,  52,   6,  53,   7,  54,   0,   0,   0,   0,   0],\n",
              "       [ 55,  56,  57,  58,  16,  59,  60,   0,   0,   0,   0],\n",
              "       [ 61,  17,  62,  18,   8,  11,  63,  64,  65,   0,   0],\n",
              "       [ 66,   6,  67,  19,  68,  69,   8,   0,   0,   0,   0],\n",
              "       [ 70,   1,  13,  14,   7,  12,  10,  71,   9,  72,  20],\n",
              "       [  4,  73,  74,   3,  75,  76,  77,   0,   0,   0,   0],\n",
              "       [  5,  78,  18,  79,  80,  81,   1,   0,   0,   0,   0],\n",
              "       [ 82,  83,  84,  19,  85,  86,   1,  87,   0,   0,   0],\n",
              "       [ 88,  89,   2,  90,  91,  92,   0,   0,   0,   0,   0],\n",
              "       [ 93,  94,  95,  96,  21,  97,  98,  16,   0,   0,   0],\n",
              "       [ 99, 100, 101, 102,   5, 103,   0,   0,   0,   0,   0],\n",
              "       [104,  20, 105,   6, 106,   0,   0,   0,   0,   0,   0],\n",
              "       [ 17, 107,   8,  21, 108, 109, 110, 111,   0,   0,   0],\n",
              "       [ 15, 112, 113, 114, 115,   4,   0,   0,   0,   0,   0],\n",
              "       [  1,   7, 116, 117, 118, 119,   0,   0,   0,   0,   0]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "max_len = max(len(i) for i in sequence)\n",
        "padding = pad_sequences(sequence,maxlen=max_len, padding='post')\n",
        "\n",
        "padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eqzsk16vXEK",
        "outputId": "b5700556-4644-4a3b-b7ea-9dba6a1af845"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "vocab_size = len(tokenizer.word_index)+1\n",
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KMqghWpYD_wV",
        "outputId": "5ba025ba-90e9-4317-cbc7-fb971e9f0830"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  1,  22,  23,   2,  24,   3,  25,  26,  27,   0,   0],\n",
              "       [  9,   4,  10,   2,   5,  28,  29,  30,  31,   0,   0],\n",
              "       [ 32,  33,  34,  35,  36,  11,  37,  38,   0,   0,   0],\n",
              "       [ 39,  40,  12,   3,  41,  42,  43,  44,  45,   0,   0],\n",
              "       [ 46,  47,  48,  13,  14,  49,  15,  50,  51,   0,   0],\n",
              "       [  2,  52,   6,  53,   7,  54,   0,   0,   0,   0,   0],\n",
              "       [ 55,  56,  57,  58,  16,  59,  60,   0,   0,   0,   0],\n",
              "       [ 61,  17,  62,  18,   8,  11,  63,  64,  65,   0,   0],\n",
              "       [ 66,   6,  67,  19,  68,  69,   8,   0,   0,   0,   0],\n",
              "       [ 70,   1,  13,  14,   7,  12,  10,  71,   9,  72,  20],\n",
              "       [  4,  73,  74,   3,  75,  76,  77,   0,   0,   0,   0],\n",
              "       [  5,  78,  18,  79,  80,  81,   1,   0,   0,   0,   0],\n",
              "       [ 82,  83,  84,  19,  85,  86,   1,  87,   0,   0,   0],\n",
              "       [ 88,  89,   2,  90,  91,  92,   0,   0,   0,   0,   0],\n",
              "       [ 93,  94,  95,  96,  21,  97,  98,  16,   0,   0,   0],\n",
              "       [ 99, 100, 101, 102,   5, 103,   0,   0,   0,   0,   0],\n",
              "       [104,  20, 105,   6, 106,   0,   0,   0,   0,   0,   0],\n",
              "       [ 17, 107,   8,  21, 108, 109, 110, 111,   0,   0,   0],\n",
              "       [ 15, 112, 113, 114, 115,   4,   0,   0,   0,   0,   0],\n",
              "       [  1,   7, 116, 117, 118, 119,   0,   0,   0,   0,   0]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c56q_CWND_t0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MVQyT5k08bbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9FE3lmgwFHf"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Input(shape=[max_len]),\n",
        "    Embedding(input_dim = vocab_size, output_dim = 20),\n",
        "    LSTM(64, return_sequences=False),\n",
        "    Dense(1, activation = 'sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frXl52FWBuyR"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTLk50naBwfw",
        "outputId": "746781b9-b1ae-4a1d-cb26-b0b62ea381b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.5625 - loss: 0.6925 - val_accuracy: 0.5000 - val_loss: 0.6927\n",
            "Epoch 2/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.5625 - loss: 0.6916 - val_accuracy: 0.5000 - val_loss: 0.6927\n",
            "Epoch 3/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.5625 - loss: 0.6907 - val_accuracy: 0.5000 - val_loss: 0.6927\n",
            "Epoch 4/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.5625 - loss: 0.6896 - val_accuracy: 0.7500 - val_loss: 0.6926\n",
            "Epoch 5/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.6250 - loss: 0.6884 - val_accuracy: 0.5000 - val_loss: 0.6925\n",
            "Epoch 6/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.7500 - loss: 0.6869 - val_accuracy: 0.5000 - val_loss: 0.6923\n",
            "Epoch 7/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8125 - loss: 0.6852 - val_accuracy: 0.5000 - val_loss: 0.6921\n",
            "Epoch 8/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8750 - loss: 0.6831 - val_accuracy: 0.5000 - val_loss: 0.6919\n",
            "Epoch 9/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.8125 - loss: 0.6807 - val_accuracy: 0.5000 - val_loss: 0.6916\n",
            "Epoch 10/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.7500 - loss: 0.6778 - val_accuracy: 0.5000 - val_loss: 0.6913\n",
            "Epoch 11/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8125 - loss: 0.6743 - val_accuracy: 0.5000 - val_loss: 0.6909\n",
            "Epoch 12/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.9375 - loss: 0.6702 - val_accuracy: 0.5000 - val_loss: 0.6904\n",
            "Epoch 13/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9375 - loss: 0.6653 - val_accuracy: 0.5000 - val_loss: 0.6898\n",
            "Epoch 14/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 1.0000 - loss: 0.6594 - val_accuracy: 0.7500 - val_loss: 0.6890\n",
            "Epoch 15/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.6523 - val_accuracy: 0.7500 - val_loss: 0.6882\n",
            "Epoch 16/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 1.0000 - loss: 0.6439 - val_accuracy: 0.7500 - val_loss: 0.6871\n",
            "Epoch 17/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 1.0000 - loss: 0.6338 - val_accuracy: 0.7500 - val_loss: 0.6858\n",
            "Epoch 18/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 1.0000 - loss: 0.6217 - val_accuracy: 0.7500 - val_loss: 0.6842\n",
            "Epoch 19/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 1.0000 - loss: 0.6072 - val_accuracy: 0.7500 - val_loss: 0.6822\n",
            "Epoch 20/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 1.0000 - loss: 0.5900 - val_accuracy: 1.0000 - val_loss: 0.6797\n",
            "Epoch 21/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 1.0000 - loss: 0.5695 - val_accuracy: 1.0000 - val_loss: 0.6765\n",
            "Epoch 22/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 1.0000 - loss: 0.5452 - val_accuracy: 1.0000 - val_loss: 0.6726\n",
            "Epoch 23/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 1.0000 - loss: 0.5166 - val_accuracy: 0.7500 - val_loss: 0.6677\n",
            "Epoch 24/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 1.0000 - loss: 0.4832 - val_accuracy: 0.7500 - val_loss: 0.6616\n",
            "Epoch 25/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.4449 - val_accuracy: 0.7500 - val_loss: 0.6539\n",
            "Epoch 26/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 1.0000 - loss: 0.4016 - val_accuracy: 0.7500 - val_loss: 0.6443\n",
            "Epoch 27/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 1.0000 - loss: 0.3540 - val_accuracy: 0.7500 - val_loss: 0.6323\n",
            "Epoch 28/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 1.0000 - loss: 0.3033 - val_accuracy: 0.7500 - val_loss: 0.6170\n",
            "Epoch 29/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 1.0000 - loss: 0.2516 - val_accuracy: 0.7500 - val_loss: 0.5978\n",
            "Epoch 30/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 1.0000 - loss: 0.2018 - val_accuracy: 0.7500 - val_loss: 0.5739\n",
            "Epoch 31/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 1.0000 - loss: 0.1567 - val_accuracy: 0.7500 - val_loss: 0.5450\n",
            "Epoch 32/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 1.0000 - loss: 0.1185 - val_accuracy: 0.7500 - val_loss: 0.5113\n",
            "Epoch 33/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 1.0000 - loss: 0.0881 - val_accuracy: 0.7500 - val_loss: 0.4738\n",
            "Epoch 34/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 1.0000 - loss: 0.0652 - val_accuracy: 0.7500 - val_loss: 0.4345\n",
            "Epoch 35/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 1.0000 - loss: 0.0485 - val_accuracy: 1.0000 - val_loss: 0.3954\n",
            "Epoch 36/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 1.0000 - loss: 0.0365 - val_accuracy: 1.0000 - val_loss: 0.3582\n",
            "Epoch 37/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 1.0000 - loss: 0.0280 - val_accuracy: 1.0000 - val_loss: 0.3237\n",
            "Epoch 38/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 1.0000 - loss: 0.0219 - val_accuracy: 1.0000 - val_loss: 0.2922\n",
            "Epoch 39/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 1.0000 - loss: 0.0175 - val_accuracy: 1.0000 - val_loss: 0.2638\n",
            "Epoch 40/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 1.0000 - loss: 0.0143 - val_accuracy: 1.0000 - val_loss: 0.2382\n",
            "Epoch 41/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 1.0000 - loss: 0.0118 - val_accuracy: 1.0000 - val_loss: 0.2152\n",
            "Epoch 42/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.0100 - val_accuracy: 1.0000 - val_loss: 0.1947\n",
            "Epoch 43/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 1.0000 - loss: 0.0085 - val_accuracy: 1.0000 - val_loss: 0.1763\n",
            "Epoch 44/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 1.0000 - loss: 0.0074 - val_accuracy: 1.0000 - val_loss: 0.1599\n",
            "Epoch 45/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 1.0000 - loss: 0.0065 - val_accuracy: 1.0000 - val_loss: 0.1451\n",
            "Epoch 46/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 1.0000 - loss: 0.0058 - val_accuracy: 1.0000 - val_loss: 0.1318\n",
            "Epoch 47/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 0.1199\n",
            "Epoch 48/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 1.0000 - val_loss: 0.1091\n",
            "Epoch 49/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 1.0000 - loss: 0.0043 - val_accuracy: 1.0000 - val_loss: 0.0993\n",
            "Epoch 50/50\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 1.0000 - loss: 0.0040 - val_accuracy: 1.0000 - val_loss: 0.0905\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7cf8ba0e9940>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "model.fit(padding, y,validation_split=0.2, epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6iDEDSntw4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc844815-df33-4146-ac0b-af064d4dde53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n",
            "negative\n"
          ]
        }
      ],
      "source": [
        "testing = '''The editing is boring, with scenes ending abruptly before establishing proper\n",
        "context. Character motivations change without explanation—the villain's sudden\n",
        "change of heart in the climax feels unearned and contradicts their established\n",
        "personality. Dialogue consists mostly of exposition dumps rather than natural\n",
        "conversation.'''\n",
        "\n",
        "\n",
        "testing = testing.lower()\n",
        "tokens = word_tokenize(testing)\n",
        "clean_tokens = [i for i in tokens if i not in string.punctuation]\n",
        "clean_tokens = [i for i in clean_tokens if i not in stopwords.words('english')]\n",
        "clean_text = ' '.join(clean_tokens)\n",
        "\n",
        "\n",
        "sequences_testing = tokenizer.texts_to_sequences([clean_text])\n",
        "\n",
        "padding_testing = pad_sequences(sequences_testing, maxlen=max_len, padding='post')\n",
        "\n",
        "prediction = model.predict(padding_testing)[0][0]\n",
        "if prediction >= 0.5:\n",
        "    print('positive')\n",
        "else:\n",
        "    print('negative')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-20yGTuKvCq7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}